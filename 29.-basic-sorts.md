# 29. Basic Sorts

Author: Josh Hug

### QA [#](broken-reference) <a href="#qa" id="qa"></a>

Linked [here](https://youtu.be/z6oW8UrAhcs).

### Check-in Exercise [#](broken-reference) <a href="#check-in-exercise" id="check-in-exercise"></a>

Linked [here](https://forms.gle/P4HTmdtDXv6X9Kdd8).

### Overview [#](broken-reference) <a href="#overview" id="overview"></a>

**Inversions.** The number of pairs of elements in a sequence <mark style="color:red;">that are out of order</mark>. An array with no inversions is ordered.

**Selection sort.** One way to sort is by selection: Repeatedly identifying the most extreme element and moving it to the end of the unsorted section of the array. The naive implementation of such an algorithm is in place.

**Steps**:

* Find smallest item.
* Swap this item to the front and ‘fix’ it.
* Repeat for unfixed items until all items are fixed.
* Demo: [https://goo.gl/g14Cit](https://goo.gl/g14Cit)

**Sort Properties**:&#x20;

Θ(N2) time if we use an array (or similar data structure).

**Naive Heapsort.** A variant of selection sort is to use a heap based PQ to sort the items. To do this, insert all items into a <mark style="color:red;">MaxPQ</mark> and then remove them one by one. <mark style="color:red;">The first such item removed is placed at the end of the array, the next item right before the end, and so forth until that last item deleted is placed in position 0 of the array.</mark> Each <mark style="color:red;">insertion and deletion takes</mark> <mark style="color:red;"></mark><mark style="color:red;">**O(log N)**</mark> <mark style="color:red;"></mark><mark style="color:red;">time</mark>, and there are N insertions and deletions, resulting in a <mark style="color:red;">O(N log N)</mark> runtime. With some more work, we can show that heapsort is θ(N log N) in the worst case. This naive version of heapsort uses θ(N) for the PQ. Creation of the MaxPQ requires O(N) memory. It is also possible to use a MinPQ instead.

<mark style="color:red;">**In Place Heapsort**</mark>**.** When sorting an array, we can avoid the θ(N) memory cost by treating the array itself as a heap. To do this,&#x20;

* Heapify the array using <mark style="color:red;">bottom-up heap</mark> construction (taking <mark style="color:red;">θ(N)</mark> time)
  * Sink nodes in reverse level order: sink(k)
  * After sinking, guaranteed that tree rooted at position k is a heap.
* Repeatedly delete the max item, swapping it with the last item in the heap.&#x20;
* In-place heap sort: [Demo](https://docs.google.com/presentation/d/1SzcQC48OB9agStD0dFRgccU-tyjD6m3esrSC-GLxmNc/edit?usp=sharing).

Over time, the heap shrinks from N items to 0 items, and the sorted list from 0 items to N items. The resulting version is also θ(N log N).

**Mergesort.** We can sort by merging, as discussed in an earlier lecture. <mark style="color:red;">Mergesort is θ(N log N) and uses θ(N) memory.</mark>

* Split items into 2 roughly even pieces.
* Mergesort each half (steps not shown, this is a recursive algorithm!)
* Merge the two sorted halves to form the final result.
  * Compare input\[i] < input\[j] (if necessary).
  * Copy smaller item and increment p and i or j.
* Mergesort: [Demo](https://docs.google.com/presentation/d/1h-gS13kKWSKd\_5gt2FPXLYigFY4jf5rBkNFl3qZzRRw/edit?usp=sharing)

Time complexity, [analysis from previous lecture](https://docs.google.com/presentation/d/1\_LhI5V5JlcRHYU55\_SF7ZHxPemBr9OVlNzj7ScYdg64/edit#slide=id.g84271d11b\_2\_77): Θ(N log N runtime)

Space complexity with aux array: Costs Θ(N) memory.

**Insertion Sort.**&#x20;

<mark style="color:red;">**Like poker**</mark>. <mark style="color:red;">For each item, insert into the output sequence in the appropriate place</mark>. <mark style="color:red;">Every swap fixes exactly one inversion</mark>.

In the best case, insertion sort takes θ(N) time. In the worst case, θ(N^2) time. More generally, <mark style="color:red;">**runtime is no worse than the number of inversions**</mark>.

**Inplace Insertion Sort**&#x20;

General strategy:&#x20;

* Repeat for i = 0 to N - 1:
  * Designate item i as the traveling item.
  * Swap item backwards until traveller is in the right place among all previously examined items.
  * In-place using swapping: [Demo](https://docs.google.com/presentation/d/10b9aRqpGJu8pUk8OpfqUIEEm8ou-zmmC7b\_BE5wgNg0/edit?usp=sharing)

<mark style="color:red;">**To Keep In Mind:**</mark>

* **For small arrays (N < 15 or so), insertion sort is fastest. (Rough idea: Divide and conquer algorithms like heapsort / mergesort spend too much time dividing, but insertion sort goes straight to the conquest)**
* Very fast for arrays that are almost sorted

**Shell’s Sort (extra slides).** Not covered on exam. Idea is to compare items that are a distance h apart from each other, starting from large h and reducing down to h=1. The last step where h=1 ensures that the array is sorted (since h=1 is just insertion sort). The earlier steps help speed things up by making long distance moves, fixing many inversions at once. Theoretical analysis of Shell’s sort is highly technical and has surprising results.

### Recommended Problems [#](broken-reference) <a href="#recommended-problems" id="recommended-problems"></a>

#### B level [#](broken-reference) <a href="#b-level" id="b-level"></a>

1. Give a best and worst case input for insertion sort.
2. Which sort do you expect to run more quickly on a reversed array, selection sort or insertion sort?

See sorting comparisons (lec30) guide for more questions that involve all of our sorting algorithms.

* [QA](broken-reference)
* [Check-in Exercise](broken-reference)
* [Overview](broken-reference)
* [Recommended Problems](broken-reference)
  * [B level](broken-reference)
